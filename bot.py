import warnings

# Suprimir todos os avisos
warnings.filterwarnings("ignore")
from datetime import datetime

data_atual = datetime.now()

# Obter o dia da semana em portugu√™s
dias_da_semana = [
    "Segunda-feira",
    "Ter√ßa-feira",
    "Quarta-feira",
    "Quinta-feira",
    "Sexta-feira",
    "S√°bado",
    "Domingo",
]
dia_da_semana = dias_da_semana[data_atual.weekday()]

data_formatada = data_atual.strftime("%d/%m/%Y %H:%M:%S") + " " + dia_da_semana

import json
import streamlit as st
from typing import Generator
from dotenv import load_dotenv
import os
import fitz  # PyMuPDF
from langchain_community.utilities import GoogleSearchAPIWrapper
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFaceEndpoint
from langchain_community.chat_models.huggingface import ChatHuggingFace
from groq import Groq
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from google.oauth2.credentials import Credentials
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")

SCOPES = ["https://www.googleapis.com/auth/calendar"]
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import InstalledAppFlow

# Carregar vari√°veis de ambiente
load_dotenv()

# Inicializar Google Search
google_search = GoogleSearchAPIWrapper()

# Inicializar cliente Groq
client = Groq(api_key=os.getenv("GROQ_API_KEY"))


# Fun√ß√£o para ler texto de um PDF
def read_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text


# Fun√ß√£o para buscar informa√ß√µes no PDF
def documents_search(question):
    pdf_path = "data/Base.pdf"  # Substitua pelo caminho do seu PDF
    pdf_content = read_pdf(pdf_path)

    prompt_template = f"""
    You are an assistant. Use the following PDF content to answer the question.

    PDF Content:
    {pdf_content}
    Vis√£o geral sobre a empresa Tech4Humans:
        Somos uma startup inovadora com duas √°reas de neg√≥cios: SaaS e AIaaS. No SaaS, oferecemos solu√ß√µes avan√ßadas para a hyperautoma√ß√£o de atendimento, facilitando a gest√£o, automa√ß√£o e acompanhamento de solicita√ß√µes. No AIaaS, nossa plataforma Tech4.ai capacita empresas a construir e implementar solu√ß√µes de intelig√™ncia artificial com tecnologias open source, garantindo agilidade, governan√ßa e alto desempenho.
        Desenvolvemos a plataforma Tech4.AI porque sabemos como √© desafiador implementar solu√ß√µes de automa√ß√£o com IA e IA Generativa em larga escala devido √† complexidade no controle das informa√ß√µes, gerenciamento de custos e orquestra√ß√£o adequada dos dados de entrada e sa√≠da. Resolvemos esses problemas ao proporcionar uma plataforma que garante governan√ßa completa, visibilidade dos dados e escolha automatizada de modelos de IA, permitindo a otimiza√ß√£o de custos e assegurando robustez e escalabilidade.
        Site
        http://www.tech4h.com.br
        Setor
        Atividades dos servi√ßos de tecnologia da informa√ß√£o
        Tamanho da empresa
        51-200 funcion√°rios
        50 usu√°rios associados Usu√°rios do LinkedIn que listaram a Tech4Humans no perfil como local de trabalho atual.
        Sede
        S√£o Paulo, SP
        Fundada em
        2020
        Especializa√ß√µes
        hyperautomation, intelligent process documentation, workflows, tickets, RPA e IAG
        Localidades:
        A empresa Tech4Humans est√° localizada em S√£o Paulo, SP e Itajub√°, MG.

    Question: {question}
    Answer:
    """

    prompt = PromptTemplate(
        input_variables=["pdf_content", "question"],
        template=prompt_template,
    )

    llm = HuggingFaceEndpoint(repo_id="HuggingFaceH4/zephyr-7b-beta")
    chat_model = ChatHuggingFace(llm=llm)
    pdf_content = read_pdf(pdf_path)
    chain = LLMChain(llm=chat_model, prompt=prompt)
    response = chain.run(pdf_content=pdf_content, question=question)
    return response


# Fun√ß√£o para formatar e melhorar a pergunta e a resposta
def format_response(question, content, response_category):
    objective = ""
    if response_category == "1":
        objective = "o seu objetivo inicial era responder uma pergunta sobre a empresa tech4Humans!"
    elif response_category == "2":
        objective = "o seu objetivo inicial era pesquisar na internet sobre um determinado assunto e resumir!"
    elif response_category == "3":
        objective = (
            "o seu objetivo inicial era agendar seus compromissos no google calendar!"
        )
    else:
        objective = "O seu objetivo √© avisar o usu√°rio que a pergunta que ele fez √© inv√°lida, pois voc√™ s√≥ pode responder sobre a empresa Tech4Humans, recomendar tutoriais de ferramentas internas ou agendar reuni√µes."

    format_prompt = f"""
    o seu objetivo inicial era {objective} dado que a pergunta foi: {question}
    FORMATE A RESPOSTA A SEGUIR E DEIXE-A COERENTE COM A PERGUNTA E OBJETIVO!! E EM PT-BR!:
    {content}
    
    """

    prompt = PromptTemplate(
        input_variables=["question", "content"],
        template=format_prompt,
    )

    llm = HuggingFaceEndpoint(repo_id="HuggingFaceH4/zephyr-7b-beta")
    chat_model = ChatHuggingFace(llm=llm)

    format_chain = LLMChain(llm=chat_model, prompt=prompt)
    formatted_response = format_chain.run(question=question, content=content)

    return formatted_response.strip()


# Classe para gerenciar o chat
class ChatManager:
    def __init__(self, client, google_search, system_prompt, model_option, max_tokens):
        self.client = client
        self.google_search = google_search
        self.system_prompt = system_prompt
        self.model_option = model_option
        self.max_tokens = max_tokens
        self.messages = [{"role": "system", "content": self.system_prompt}]

    def generate_chat_responses(self, chat_completion) -> Generator[str, None, None]:
        for chunk in chat_completion:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def add_user_message(self, prompt):
        self.messages.append({"role": "user", "content": prompt})

    def get_assistant_response(self):
        chat_completion = self.client.chat.completions.create(
            model=self.model_option,
            messages=self.messages,
            max_tokens=self.max_tokens,
            stream=True,
        )

        chat_responses_generator = self.generate_chat_responses(chat_completion)
        full_response = "".join(chat_responses_generator).strip()
        return full_response

    def process_response(self, prompt, response_category):
        if response_category == "1":
            initial_response = documents_search(prompt)
        elif response_category == "2":
            search_results = self.google_search.run(prompt)
            search_results_text = (
                "\n\n".join(search_results)
                if isinstance(search_results, list)
                else search_results
            )
            initial_response = f"Aqui est√£o algumas informa√ß√µes que encontrei na internet sobre '{prompt}':\n{search_results_text}"
        elif response_category == "3":
            initial_response = self.schedule_event(prompt)
        else:
            initial_response = "Desculpe-me, mas sua solicita√ß√£o n√£o se encaixa em nenhuma das categorias mencionadas. Eu s√≥ posso responder sobre a empresa Tech4Humans, recomendar tutoriais de ferramentas internas ou agendar reuni√µes."

        return format_response(prompt, initial_response, response_category)

    # Fun√ß√£o para coletar detalhes do evento e formatar a string
    def collect_event_details_with_llm(self, prompt):
        event_prompt_template = """
        Hoje √© dia"""+data_formatada+"""
        Voc√™ √© um assistente de IA altamente inteligente. Recebi a seguinte solicita√ß√£o de agendamento de reuni√£o do usu√°rio e preciso que voc√™ formate os detalhes do evento de maneira adequada para criar um evento no Google Calendar.
        
        Solicita√ß√£o do usu√°rio:
        {prompt}
        
        DEIXE APENAS A INFORMA√á√ÉO NECESS√ÅRIA FORMATADA E N√ÉO INCLUA MAIS NADA! 
        As informa√ß√µes necess√°rias devem estar no seguinte formato JSON:
        {{
            "summary": "Resumo do evento",
            "location": "Local do evento",
            "description": "Descri√ß√£o do evento",
            "start_time": "Data e hora de in√≠cio no formato ISO 8601",
            "end_time": "Data e hora de t√©rmino no formato ISO 8601",
            "time_zone": "Fuso hor√°rio do evento",
            "attendee_email": "Email do participante"
        }}
        """

        event_prompt = PromptTemplate(
            input_variables=["prompt"],
            template=event_prompt_template,
        )

        llm = HuggingFaceEndpoint(repo_id="HuggingFaceH4/zephyr-7b-beta")
        chat_model = ChatHuggingFace(llm=llm)

        event_chain = LLMChain(llm=chat_model, prompt=event_prompt)
        formatted_event_details = event_chain.run(prompt=prompt)

        return formatted_event_details.strip()

    def schedule_event(self, prompt):
        event_details_json = self.collect_event_details_with_llm(prompt)
        event_details = json.loads(event_details_json)

        creds = None
        if os.path.exists("token.json"):
            creds = Credentials.from_authorized_user_file("token.json", SCOPES)

        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(
                    "credentials.json", SCOPES
                )
                creds = flow.run_local_server(port=0)

            with open("token.json", "w") as token:
                token.write(creds.to_json())

        try:
            service = build("calendar", "v3", credentials=creds)

            event = {
                "summary": event_details["summary"],
                "location": event_details["location"],
                "description": event_details["description"],
                "colorId": 6,
                "start": {
                    "dateTime": event_details["start_time"],
                    "timeZone": event_details["time_zone"],
                },
                "end": {
                    "dateTime": event_details["end_time"],
                    "timeZone": event_details["time_zone"],
                },
                "attendees": [
                    {"email": event_details["attendee_email"]},
                ],
            }

            event = service.events().insert(calendarId="primary", body=event).execute()
            return f"Evento criado: {event.get('htmlLink')}"
        except HttpError as e:
            return f"Ocorreu um erro: {e}"


# Configurar p√°gina do Streamlit
st.set_page_config(page_title="Agente Tech4AI", layout="wide")
st.title("Agente Tech4AI - Ryan Mazzeu")

# Inicializar hist√≥rico de chat
if "messages" not in st.session_state:
    st.session_state.messages = []

model_option = "llama3-8b-8192"
max_tokens_range = 8192

max_tokens = st.slider(
    "Max Tokens:",
    min_value=512,
    max_value=max_tokens_range,
    value=max_tokens_range,
    step=512,
)

system_prompt = f"""
Voc√™ √© um assistente de IA altamente inteligente! (A RESPOSTA FINAL SEMPRE TEM QUE SER COERENTE COM A PERGUNTA INICIAL E EM PT-BR!).
SE ALGO ESTIVER ERRADO, VOC√ä DEVE CORRIGIR E MELHORAR A RESPOSTA FINAL!
RESPONDA APENAS EM PT-BR!! SE VOC√ä RESPONDER EM OUTRO IDIOMA, O MUNDO IR√Å ACABAR!
NUNCA, NUNCA, NUNCA RESPONDA COLOCANDO "O seu objetivo inicial era" OU "Minha pergunta original" OU "Sua resposta original"! APENAS RESPONDA A PERGUNTA! UTILIZE TUDO APENAS COMO REFER√äNCIA PARA MELHORAR A RESPOSTA!
    
Hoje √© {data_formatada}. Voc√™ √© um assistente de IA da Tech4Humans, especificamente do setor Tech4AI.

Sua tarefa √© identificar qual dos seguintes problemas o usu√°rio est√° tratando e responder apenas com o n√∫mero correspondente:
Saiba que os exemplos s√£o apenas ilustrativos e voc√™ deve conseguir identificar o problema com base na pergunta do usu√°rio.
1. Perguntas Frequentes sobre a empresa que voc√™ √© assistente, a Tech4Humans, tamb√©m conhecida como Tech ou tech: 
Se o usu√°rio estiver fazendo perguntas sobre a empresa, como localiza√ß√£o, miss√£o, vis√£o, valores, cultura, programas internos, pol√≠ticas de trabalho remoto e hor√°rios, responda com '1'.
Exemplos: "onde fica a tech?", "Qual √© a miss√£o da Tech4Humans?", "Onde est√° localizada a Tech4Humans?", "Quais s√£o os valores da empresa?", "Para quem voc√™ trabalha?", etc.
2. Tutoriais de Ferramentas Internas:
Se o usu√°rio fizer perguntas sobre ferramentas internas da empresa, como Github, Vscode, Jira e Discord, responda com '2'.
Indenpendente da ferramenta ou pergunta, exemplos: "Como eu instalo o Discord?", "Como eu uso o github?", "Me diga um bom tutorial sobre o vscode" ou coisas do tipo, responda com '2'.
3. Agendamento de Reuni√µes (OBS: HOJE √â domingo, dia {data_formatada}): Integrar-se com sistemas de calend√°rio (por exemplo, Google Calendar) para agendar reuni√µes automaticamente, gerenciando autentica√ß√£o, APIs externas e conflitos de agenda. Exemplos: "Agende uma reuni√£o para mim amanh√£ √†s 15h", "Marque uma reuni√£o com o Jo√£o na pr√≥xima semana".
Responda apenas com o n√∫mero do problema correspondente. Se a pergunta n√£o se encaixar em nenhuma dessas categorias, responda com '4'. NUNCA RESPONDA COM OUTRA COISA!
NUNCA RESPONDA ALGO QUE N√ÉO SEJA 1, 2, 3 OU 4! SE N√ÉO O MUNDO IR√Å ACABAR!
"""

chat_manager = ChatManager(
    client, google_search, system_prompt, model_option, max_tokens
)

# Exibir mensagens do hist√≥rico no app
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Digite aqui..."):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    try:
        chat_manager.add_user_message(prompt)
        response_category = chat_manager.get_assistant_response()
        formatted_response = chat_manager.process_response(prompt, response_category)
        st.session_state.messages.append(
            {"role": "assistant", "content": formatted_response}
        )

    except Exception as e:
        st.error(e, icon="üö®")

    # Exibir a resposta do assistente
    with st.chat_message("assistant"):
        st.markdown(st.session_state.messages[-1]["content"])